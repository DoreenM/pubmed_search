---
title: "RSV Search Results Markdown"
date : 20/12/2017
author: Doreen Mokeira
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## About
The Rmarkdown shows instructions on text mining of pubmed documents to narrow down search results. Pubmed is a public database with published journals and other documents made available by NIH. Package RISmed is used to download abstracts and save them for further processing. Package tm is used for text mining using natural language processing. Other packages like tidytext will be used to tidy the processed data to make it more usable.


## Method

Firstly, install all packages required by checking if they are installed already.
Load the libraries 


## Rismed Usage

Rismed library has a EUtils- to summarise and retrieve results as S4 class objects.


```{r, echo=FALSE}
# install packages if required

if(!require(ggplot2)) install.packages("ggplot2",repos = "http://cran.us.r-project.org")
if(!require(RISmed)) install.packages("RISmed",repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse",repos = "http://cran.us.r-project.org")

# load libraries of installed packages

library(ggplot2)
library(RISmed)
library(tidyverse)

# List of search terms related to my topic of study

search_topic <- ' Respiratory Syncytial Virus '
search_topic2 <- 'rsv'
search_topic3 <- 'Transmission Bottleneck'
search_topic4 <- 'hrsv'

#Search queries to execute

search_query <- EUtilsSummary(search_topic,retmax =5000, mindate =1960,maxdate=2017)
summary(search_query)
search_query2 <- EUtilsSummary(search_topic2,retmax =5000, mindate =1960,maxdate=2017)
summary(search_query2)
search_query3 <- EUtilsSummary(search_topic3,retmax =5000, mindate =1960,maxdate=2017)
summary(search_query3)
search_query4 <- EUtilsSummary(search_topic4,retmax =5000, mindate =1960,maxdate=2017)
summary(search_query4)


#Get pubmed abstracts and store in data frame

records1 <- EUtilsGet(search_query)
records2 <- EUtilsGet(search_query2)
records3 <- EUtilsGet(search_query3)
records4 <- EUtilsGet(search_query4)


#Construct dataframe with columns PMID, Title,Abstract,Year

pubmed_data1 <- data.frame('Pid' = PMID(records1),
                          'Title' = ArticleTitle(records1),
                          'Abstract' = AbstractText(records1),
                          'Year'= YearPubmed(records1))

pubmed_data2 <- data.frame('Pid' = PMID(records2),
                          'Title' = ArticleTitle(records2),
                          'Abstract' = AbstractText(records2),
                          'Year'= YearPubmed(records2))

pubmed_data3 <- data.frame('Pid' = PMID(records3),
                          'Title' = ArticleTitle(records3),
                          'Abstract' = AbstractText(records3),
                          'Year'= YearPubmed(records3))

pubmed_data4 <- data.frame('Pid' = PMID(records4),
                          'Title' = ArticleTitle(records4),
                          'Abstract' = AbstractText(records4),
                          'Year'= YearPubmed(records4))

# Merge dataframes and get unique documents incase abstracts overlap


pubmed_data <- rbind(pubmed_data1,pubmed_data2,pubmed_data3,pubmed_data4)
pubmed_data <- unique(pubmed_data)

write.csv(pubmed_data,file = "data/pubmed_data.csv")

``` 

```{r,echo=FALSE}
# install packages if required

if(!require(tidyverse)) install.packages("tidyverse",repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm" , repos = "http://cran.us.r-project.org")
if(!require(SnowballC)) install.packages("SnowballC" , repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud" , repos = "http://cran.us.r-project.org")

# load libraries of installed packages

library("tm")
library("SnowballC")
library(ggplot2)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)


#Load pubmed_data.csv
pubmed_data <- read.csv("data/pubmed_data.csv")
View(pubmed_data)

# number of documents on RSV per year

count <- as.data.frame(table(pubmed_data$Year))
names(count)[names(count)=="Var1"] <- "Year"
names(count)[names(count)=="Freq"] <- "number"

jpeg(filename = "figures/documentperYear.jpeg")
ggplot(count, aes(x = Year, y= number)) +
  geom_bar(stat = 'identity')
dev.off()
# Generate corpus to use for text mining

pubmed_records <- Corpus(VectorSource(pubmed_data$Abstract))

# Convert the text to lower case

pubmed_records<-tm_map(pubmed_records, content_transformer(tolower))

# Remove digits/numbers

pubmed_records<-tm_map(pubmed_records, removeNumbers)

#stem words 

pubmed_records <- tm_map(pubmed_records, stemDocument)

# Remove english common stopwords

pubmed_records <- tm_map(pubmed_records, removeWords, stopwords("english"))

# Remove punctuations and white spaces

pubmed_records <- tm_map(pubmed_records, removePunctuation)
pubmed_records <- tm_map(pubmed_records, stripWhitespace)


# Remove additional stopwords
pubmed_records<- tm_map(pubmed_records, removeWords, c("clintonemailcom", "stategov", "hrod"))

# Corpus ready so generate TermDocumentMatrix
dtm <- TermDocumentMatrix(pubmed_records)
inspect(dtm)

#Convert TermDocumentMatrix to type matrix 
#generate Dataframe of word verses frequency of word in corpus

dtMatrix <- as.matrix(dtm)
v <- sort(rowSums(dtMatrix),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
freqr <- colSums(as.matrix(dtm))

# WordVector with words that exist in corpus at a frequency of more than 500
# Will use this to filter the terms

wordVector <- subset(d,freqr<2500)

#tidy the text using tidytext and map id to document
a<-tidy(dtm)
a<- a[,c("document","term", "count")]
pubmed_data$id <- seq(1,length.out = nrow(pubmed_data), by=1)

#document id conversion from character to numeric to be able to merge the dataframes
a$document <- as.numeric(a$document)

colnames(a)[1] <- "id"
mergedc <- merge(pubmed_data,a , by= "id")

write.csv(mergedc , file = "data/Merged_document.csv")

TempMerge <- mergedc %>% 
  select(-Abstract,-Title,-id) %>%
  filter(count != 1, !term %in% c("≤","•","≥","··","°c","µm","μm","€","∼", term == wordVector)) %>% 
  group_by(Year, term) %>%
  summarise(n=n()) 

plot1 <- ggplot(TempMerge,aes(factor(Year),n)) + 
  geom_boxplot() +
  #geom_jitter() +
  scale_y_log10() +
  labs(y="log10 words", x="Year",title="") +
  theme_minimal()

jpeg(filename = "figures/WordsPerYear.jpeg")
plot1
dev.off()


wf=data.frame(term=names(freqr),occurrences=freqr)

p <- ggplot(subset(wf, freqr>500), aes(term, occurrences))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_blank(),
               axis.title.x = element_blank(),
               axis.ticks.x = element_blank())

jpeg(filename = "figures/word_frequencies.jpeg")
p
dev.off()


# Generate the WordCloud
 par(bg="white")
png(file="figures/WordCloud.png",width=1000,height=700, bg="white")
#wordcloud(wordVector$word, wordVector$freq, col=terrain.colors(length(d$word), alpha=0.9), random.order=FALSE, rot.per=0.3 )
title(main = "a few terms in a wordCloud", font.main = 1, col.main = "cornsilk3", cex.main = 1.5)
dev.off()

 png("figures/AnotherWordCloud.png",width = 800,height = 500)
 wordcloud(d$word,d$freq, min.freq=7000)
 dev.off()
 
 
```



